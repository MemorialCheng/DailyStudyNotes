___Here are the details of January 2020.___

# Date20200101
今天是新年第一天
辗转折腾，2019年已落下帷幕，好吧，那就让奇迹在2020年从此刻开始见证……
## 今日小结
### softmax回归从零开始实现：
1. 获取和读取数据;
2. 初始化模型参数：设置输入、偏置、输出参数大小；
3. 实现softmax函数运算方法；
4. 定义模型：也即算法的计算公式代码化；
5. 定义损失函数：
6. 计算分类准确率：由于标签类型为整数，这里可能会用到.mean().asscalar()函数，X.asscalar()是将向量X转换成标量，且向量X只能为一维含单个元素的向量;
7. 训练模型；
![image](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/learningImages/%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.png)
8. 预测。

### MLP多层感知机从零开始实现：
1. 获取和读取数据;
2. 定义模型参数：设置输入层、隐藏层、输出层参数；
3. 定义激活函数；
4. 定义损失函数；
5. 训练模型；

# Date20200102

## 今日小结
### MLP多层感知机使用Gluon来实现
1. 获取和读取数据；
2. 定义模型：直接调gluon现有接口，传参即可，激活函数也作为参数定义；
```python
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'), nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
```
3. 定义损失函数：直接调gluon接口，loss = gloss.SoftmaxCrossEntropyLoss();
4. 训练模型；



