---
# 总结算法常见面试题，简答
---
# 1 决策树
*参考百面机器学习P63-70,统计机器学习P70-86*   
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。  
一般而言，决策树的生成包括特征选择、树的构造、树的剪枝三个过程。
## 1.1 特征选择
特征选择的准则是信息增益或信息增益比。
### 1.1.1 信息增益(information gain)
__熵(entropy)__ H(X)是度量样本集合纯度(不确定性)的一种常用指标。熵越小，纯度越高。  
H(X) = -求和(PilogPi)  
__条件熵__ H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。  
H(Y|X) = 求和PiH(H|X=xi)  
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分布称为经验熵和经验条件熵。  
__信息增益__ 就是经验熵和经验条件熵之差。
<img>(https://github.com/MemorialCheng/EverybodyEveryday/upload/master/leetcode_offer/picture/entropy.jpg size=400)  
决策树根据信息增益选择特征。给定训练数据集D和特征A,经验熵H(D)表示对数据集D进行分类的不确定性。经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。  
两者之差，即信息增益g(D,A)就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。 __信息增益大的特征具有更强的分类能力.__  

选择方法就是：对训练数据（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

### 1.1.2 信息增益比(information gain ratio)
根据信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以校正该问题。
定义：特征A对于数据集D的信息增益比为其信息增益g(D,A)与数据集D关于特征A的熵H(D)之比，
gR(D,A)=g(D,A)HA(D)


