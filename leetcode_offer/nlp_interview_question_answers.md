---
# 总结算法常见面试题，简答
---
# 1 决策树
*参考百面机器学习P63-70,统计机器学习P70-86*   
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。  
__一般而言，决策树的生成包括特征选择、树的构造、树的剪枝三个过程。__
## 1.1 特征选择
特征选择的准则是信息增益或信息增益比。
### 1.1.1 信息增益(information gain)
__熵(entropy)__ H(X)是度量样本集合纯度(不确定性)的一种常用指标。熵越小，纯度越高。  
H(X) = -求和(PilogPi)  
__条件熵__ H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。  
H(Y|X) = 求和PiH(H|X=xi)  
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分布称为经验熵和经验条件熵。  
__信息增益__ 就是经验熵和经验条件熵之差。
<img>(https://github.com/MemorialCheng/EverybodyEveryday/upload/master/leetcode_offer/picture/entropy.jpg size=400)  
决策树根据信息增益选择特征。给定训练数据集D和特征A,经验熵H(D)表示对数据集D进行分类的不确定性。经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。  
两者之差，即信息增益g(D,A)就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。 __信息增益大的特征具有更强的分类能力.__  

选择方法就是：对训练数据（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

### 1.1.2 信息增益比(information gain ratio)
根据信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以校正该问题。
定义：特征A对于数据集D的信息增益比为其信息增益g(D,A)与数据集D关于特征A的熵H(D)之比，
gR(D,A)=g(D,A)HA(D)

## 1.2 决策树的生成
决策树学习的经典算法：ID3,C4.5,CART
### 1.2.1 ID3
ID3算法的核心是在决策树的各个结点上应用 __信息增益__ 标准选择特征，递归的构建决策树。  
缺点：只有树的生成，容易产生过拟合。  

### 1.2.2 C4.5
C4.5算法和ID3相似，应用 __信息增益比__ 来选择特征。

## 1.3 决策树的剪枝(pruning)
决策树面临一个严重问题是过拟合。  
过拟合原因：学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。  
解决办法：考虑决策树进行简化，这个简化的过程叫做剪枝。减掉一些枝叶，提升模型的泛化能力。  
决策树的剪枝通常有两种方法，预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。  

### 1.3.1 预剪枝
预剪枝，即在生成决策树的过程中提取停止树的增长。  
### 1.3.2 后剪枝
后剪枝，即在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。  




