---
# 总结算法常见面试题，简答
---
# 1 决策树
*参考百面机器学习P63-70,统计机器学习P70-86*   
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。  
__一般而言，决策树的生成包括特征选择、树的构造、树的剪枝三个过程。__
## 1.1 特征选择
特征选择的准则一般是信息增益、信息增益比或Gini指数。
### 1.1.1 信息增益(information gain)
__熵(entropy)__ H(X)是度量样本集合纯度(不确定性)的一种常用指标。熵越小，不确定性越小，纯度越高。（熵值与不确定性成正比，与纯度成反比）  
H(X) = -求和(PilogPi)  
__条件熵__ H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。  
H(Y|X) = 求和PiH(H|X=xi)  
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分布称为经验熵和经验条件熵。  
__信息增益__ 就是经验熵和经验条件熵之差。
![img](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/leetcode_offer/picture/entropy.jpg)  
决策树根据信息增益选择特征。给定训练数据集D和特征A,经验熵H(D)表示对数据集D进行分类的不确定性。经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。  
两者之差，即信息增益g(D,A)就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。 __信息增益大的特征具有更强的分类能力.__  

选择方法就是：对训练数据（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

### 1.1.2 信息增益比(information gain ratio)
根据信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以校正该问题。
定义：特征A对于数据集D的信息增益比为其信息增益g(D,A)与数据集D关于特征A的熵H(D)之比，
gR(D,A)=g(D,A)HA(D)  
![img](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/leetcode_offer/picture/entropy_ratio.jpg)  

## 1.2 决策树的生成
决策树学习的经典算法：ID3,C4.5,CART
### 1.2.1 ID3
ID3算法的核心是在决策树的各个结点上应用 __信息增益__ 标准选择特征（选择最大的信息增益），递归的构建决策树。  
缺点：
- 只有树的生成，容易产生过拟合;
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于1；
- 只能用于处理离散分布的特征；
- 没有考虑缺失值。

### 1.2.2 C4.5
C4.5算法和ID3相似，应用 __信息增益比__ 来选择特征（选择最大的信息增益比）。
优点：
- 引入悲观剪枝策略进行后剪枝；
- 信息增益比作为特征现在标准，克服了ID3对取值较多的特征有偏好这一缺点；
- 可以处理连续型变量，将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；
- 对于缺失值的处理可以分为两个子问题：
问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）  
问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）  
针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；  
针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。  

缺点：
- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。

### 1.2.3 CART (Classification and Regression Tree)
Gini描述的是数据的纯度，与信息熵含义类似（选择Gini最小的特征）：  
Gini(D) = 1 - 求和(Pi)^2  
CART在每一次迭代过程选择基尼指数最小的特征及其对应的切分点进行分类。  
是一颗二叉树，每一步将数据按特征A的取值切分为两份，分别进入左右子树。
Gini(D|A) = 求和Pi*Gini(Di)  
![img](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/leetcode_offer/picture/gini.jpg)  

## 1.3 决策树的剪枝(pruning)
决策树面临一个严重问题是过拟合。  
过拟合原因：学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。  
解决办法：考虑决策树进行简化，这个简化的过程叫做剪枝。减掉一些枝叶，提升模型的泛化能力。  
决策树的剪枝通常有两种方法，预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。  

### 1.3.1 预剪枝
预剪枝，即在生成决策树的过程中提取停止树的增长。  
预剪枝可能存在不同类别的样本同时存在节点中，常用多数投票的原则判断该节点所属类别。  
对于何时停止决策树生长有以下几种方法：
- 当树到达一定深度时，停止树的生长。
- 当到达当前节点的样本数量小于某个阈值时，停止树的生长。
- 计算每次分裂对测试集的准确度提升，当小于某个阈值的时，停止树的生长。

预剪枝的优缺点：
- 优点：思想直接、算法简单、效率高等特点，适合解决大规模问题。
- 缺点：如何准确的估计何时停止树的生长（深度或者阈值判断），针对不同问题会有很大差别；预剪枝存在一定局限性，有欠拟合的风险。

### 1.3.2 后剪枝
后剪枝，即在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。   

后剪枝过程从底层向上计算是否剪枝，将子树删除，用另一个叶子结点替代，该结点的类别同样按照多数投票原则判断所属类别。  
后剪枝的方法有：
- 计算在测试集上的准确度，如果剪枝后准确度有所提升，则进行剪枝。  

后剪枝的优缺点：
- 优点：相较于预剪枝，可以得到泛化能力更强的决策树。
- 缺点：时间开销大。

## 1.4 ID3、C4.5、CART区别
- 特征选择标准不同：ID3根据信息增益，C4.5根据信息增益比，CART根据Gini指数；
- 数据样本不同：ID3只能处理离散型变量，C4.5和CART还可以处理连续型变量；
- 应用：ID3和C4.5只能用于分类任务，CART还能用于回归任务（回归树使用最小平方误差准则）；
- 实现细节：ID3对缺失值敏感，C4.5采用概率权重的方法处理缺失值，CART用的是surrogate splits（替代划分）的方式来处理缺失值;ID3和C4.5每个结点可以产生多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，最后会形成一颗二叉树，且每个特征可被重复使用；
- 优化过程：ID3和C4.5通过剪枝来权衡树的准确性和泛化能力，CART直接利用全部数据发现所有可能的树结构进行对比。  

## 1.5 信息熵和基尼指数区别与联系
基尼指数是信息熵中-logP在P=1处一阶泰勒展开后的结果！所以两者都可以用来度量数据集的纯度，用于描述决策树节点的纯度！  
[参考1](https://blog.csdn.net/YE1215172385/article/details/79470926)  
[参考2](https://zhuanlan.zhihu.com/p/85731206)

# 2 LR逻辑回归
[参考链接](https://www.bilibili.com/video/BV17s411j7s1/?spm_id_from=autoNext)

![image](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/leetcode_offer/picture/lr.png)

![image](https://github.com/MemorialCheng/EverybodyEveryday/blob/master/leetcode_offer/picture/lr1.png)

# 3 集成学习-AdaBoost、GBDT、XGBoost、Lightgbm、随机森林
## 3.1 Bagging
- 常见的有随机森林，基分类器是决策树。  
- 主要是通过减小方差来提升弱分类器性能。  
- 各弱分类器之间不存在强依赖关系，相反bagging就是追求模型之间独立性。比如随机森林，每次选取节点分裂最优属性，就是为了避免弱分类之间过强的相关性；通过训练集的重采样也能带来弱分类器之间的一定独立性，从而降低Bagging后模型的方差。

## 3.2 Boosting
- 常见的有AdaBoost、GBDT、XGBoost、Lightgbm，目前最火的xgboost。
- 主要通过减小偏差来提升弱分类器性能。  

### 3.2.1 gbdt的残差为什么用负梯度代替？
(1) 负梯度永远是函数下降最快的方向，自然也是gbdt目标函数下降最快的方向，所以用梯度去拟合首先是没什么问题的
		（并不是拟合梯度，只是用梯度去拟合，发现好多人搞错）；gbdt本来中的g代表gradient，本来就是用梯度拟合；

(2) 用残差去拟合，只是目标函数是均方误差的一种特殊情况，这个特殊情况跟CART拟合残差一模一样，使得看起来就拟合残差合情合理。

(3) 为啥要去用梯度拟合不用残差？代价函数除了loss还有正则项，正则中有参数和变量，很多情况下只拟合残差loss变小但是正则变大，代价函数不一定就小，
		这时候就要用梯度啦，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向。
    

[xgboost常见面试题](https://blog.csdn.net/weixin_38753230/article/details/100571499?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)

# 4 参数计算
## 4.1 LSTM参数计算  
[参考链接](https://www.zhihu.com/question/268956632/answer/523742738)

## 4.2 Bert_Base参数计算  
[参考链接](https://blog.csdn.net/weixin_43922901/article/details/102602557)



# 5 CRF

[最通俗易懂的BiLSTM-CRF模型中的CRF层介绍](https://zhuanlan.zhihu.com/p/44042528)  

Tensorflow1.x实现BiLstm+CRF:[讲解](https://blog.csdn.net/u013963380/article/details/109270714)|[代码](https://github.com/xudongMk/AwesomeNLPBaseline/tree/main/named_entity_recognition )


[如何解决样本不均衡](https://www.jianshu.com/p/76dce1fca85b)


