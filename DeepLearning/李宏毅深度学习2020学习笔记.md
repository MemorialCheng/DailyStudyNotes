# 李宏毅深度学习2020学习笔记
---
SGD

SGDM:stable,little generalization gap,better convergence

Adagrad

RMSProp

Adam：fast training,larg generalization gap,unstable

SWATS: Begin with Adam(fast),end with SGDM
