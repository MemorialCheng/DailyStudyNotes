# 4 CNN卷积神经网络
图像分类常用经典算法：
LeNet  1988年;
AlexNet;
VggNet;
GoogleNet;
ResNet;

目标检测算法：
Faster R-CNN;
SSD;
Yolo v3	2018年;

## 一般架构
Convolution 卷积层：进行特征提取，拥有局部感知机制，权值共享；

Pooling 池化层：对特征图进行稀疏处理，减少数据运算；

Full connected 全连接层

## 二维input
### 场景1：步长=1

__假设input为n\*n，pading填充为 p，filter为f\*f，则将output为(n + 2p - f + 1) * (n + 2p - f + 1)__

如果想要input与output相同size,则p=(f-1)/2. It's actually almost always odd.

### 场景2：步长stride>1

公式：__[(n + 2p - f)/s + 1] * [(n + 2p - f)/s +1]__

## 三维input

### 存在多个filter
Nf表示filters个数

公式：__[(n + 2p - f)/s + 1] * [(n + 2p - f)/s +1] * Nf__
# 2 经典神经网络
## 2.1 LeNet-5
1988年 cov1,avg pool1,cov1,avg pool1,FCFC,softmax


## 2.2 AlexNet
首次使用GPU进行网络加速训练；使用了RuLU激活函数，而不是传统的Sigmoid激活函数以及Tanh;使用了LRN局部相应归一化;在全连接层的前两层使用了Dropout随机失活神经元操作，减少过拟合。

<img src="https://github.com/MemorialCheng/EverybodyEveryday/blob/master/DeepLearning/images/AlexNet01.png" width = "800">

<img src="https://github.com/MemorialCheng/EverybodyEveryday/blob/master/DeepLearning/images/AlexNet.png" width = "800">

## 2.3 Vgg-16
2014年

<img src="https://github.com/MemorialCheng/EverybodyEveryday/blob/master/DeepLearning/images/Vgg-16.png" width = "800">

<img src="https://github.com/MemorialCheng/EverybodyEveryday/blob/master/DeepLearning/images/vgg_16.png" width="800">

## 2.4 GoogleNet
2014年 GoogleNet团队提出。优点：引入了Inception结构（融合不同尺度的特征信息）；使用1x1卷积核进行降维以及映射处理；
添加两个辅助分类器帮助训练；丢去全连接层，使用平均池化层（大大减少模型参数）。

##  2.5 残差网络(ResNets)（Residual Networks）
非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。这节课我们学习跳跃连接（Skip connection），它可以从某一层网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层。

ResNets是由残差块（Residual block）构建的

> 梯度消失、梯度爆炸 

> 退化问题(degradation problem)

